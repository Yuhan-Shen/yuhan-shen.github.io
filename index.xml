<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yuhan Shen</title>
    <link>https://yuhan-shen.github.io/</link>
      <atom:link href="https://yuhan-shen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yuhan Shen</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Yuhan Shen</copyright><lastBuildDate>Sat, 28 Sep 2019 00:28:59 -0400</lastBuildDate>
    <image>
      <url>https://yuhan-shen.github.io/img/icon-192.png</url>
      <title>Yuhan Shen</title>
      <link>https://yuhan-shen.github.io/</link>
    </image>
    
    <item>
      <title>Skills</title>
      <link>https://yuhan-shen.github.io/skills/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accomplish&amp;shy;ments</title>
      <link>https://yuhan-shen.github.io/accomplishments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/accomplishments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://yuhan-shen.github.io/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Featured Publications</title>
      <link>https://yuhan-shen.github.io/featured/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/featured/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Popular Topics</title>
      <link>https://yuhan-shen.github.io/tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/tags/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[2019/09] Starting PhD program at Northeastern University</title>
      <link>https://yuhan-shen.github.io/post/neu/</link>
      <pubDate>Sat, 28 Sep 2019 00:28:59 -0400</pubDate>
      <guid>https://yuhan-shen.github.io/post/neu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection</title>
      <link>https://yuhan-shen.github.io/publication/interspeech2019_shen/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/publication/interspeech2019_shen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection</title>
      <link>https://yuhan-shen.github.io/publication/interspeech2019_he/</link>
      <pubDate>Sat, 14 Sep 2019 00:55:01 -0400</pubDate>
      <guid>https://yuhan-shen.github.io/publication/interspeech2019_he/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[2019/08] One paper is accepted by DCASE Workshop 2019</title>
      <link>https://yuhan-shen.github.io/post/dcase2019/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/post/dcase2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[2019/07] Won the 2nd place in Task 2 of DCASE Challenge 2019</title>
      <link>https://yuhan-shen.github.io/post/dcase2019_challenge/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/post/dcase2019_challenge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Audio tagging with noisy labels and minimal supervision</title>
      <link>https://yuhan-shen.github.io/project/audio_tagging/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/project/audio_tagging/</guid>
      <description>&lt;p&gt;This is a task of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. It aims to develop a well-performed audio tagging system using a small amount of manually-labeled data and a large quantity of noisy-labeled data. We took part in this competition and won the 2nd place. To achieve state-of-the-art performance, we mainly adopted the following strategies:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We used mixup and SpecAugment for data augmentation.&lt;/li&gt;
&lt;li&gt;We designed a sigmoid-softmax activation structure to deal with sparse multi-label classification.&lt;/li&gt;
&lt;li&gt;We proposed a staged training strategy to learn from nose data.&lt;/li&gt;
&lt;li&gt;We applied post-processing method that normalizes output scores for each sound class.&lt;/li&gt;
&lt;li&gt;We adopted ensemble method that averages models learned with multiple neural networks and acoustic features.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>[2019/06] Two papers are accepted by Interspeech 2019</title>
      <link>https://yuhan-shen.github.io/post/interspeech2019/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/post/interspeech2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rare sound event detection</title>
      <link>https://yuhan-shen.github.io/project/sound_event_detection/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/project/sound_event_detection/</guid>
      <description>&lt;p&gt;We propose a temporal-frequential attention model for sound event detection (SED). Our network learns how to listen with two attention models: a temporal attention model and a frequential attention model. Proposed system learns when to listen using the temporal attention model while it learns where to listen on the frequency axis using the frequential attention model. With these two models, we attempt to make our system pay more attention to important frames or segments and important frequency components for sound event detection. Our proposed method is demonstrated on the task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge and outperforms state-of-the-art methods.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level Attention Mechanism for Home Activity Monitoring</title>
      <link>https://yuhan-shen.github.io/publication/isspit2018/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/publication/isspit2018/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
