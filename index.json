[{"authors":["admin"],"categories":null,"content":"Yuhan Shen is a PhD student in Computer Science in Khoury College of Computer Sciences at Northeastern University, Boston, MA. He has broad interests in weakly-supervised and unsupervised machine learning, computer vision and multi-modal learning. He is currently working on video segmentation and video summarization, under the supervision of Professor Ehsan Elhamifar. He was also co-advised by Professor Lu Wang.\nBefore joining Northeastern University, he received his bachelor\u0026rsquo;s degree from Department of Electronic Engineering at Tsinghua University in China in 2018. He also worked as a research assistant in Speech and Audio Technology Lab at Tsinghua University under the guidance of Professor Wei-Qiang Zhang. \n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1569703119,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yuhan-shen.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yuhan Shen is a PhD student in Computer Science in Khoury College of Computer Sciences at Northeastern University, Boston, MA. He has broad interests in weakly-supervised and unsupervised machine learning, computer vision and multi-modal learning. He is currently working on video segmentation and video summarization, under the supervision of Professor Ehsan Elhamifar. He was also co-advised by Professor Lu Wang. Before joining Northeastern University, he received his bachelor\u0026rsquo;s degree from","tags":null,"title":"Yuhan Shen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"a36058eafe01cd26b16c80f89806a069","permalink":"https://yuhan-shen.github.io/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/skills/","section":"","summary":"","tags":null,"title":"Skills","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"4f5622892cf36115c4fd6a9793b9c9d1","permalink":"https://yuhan-shen.github.io/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/accomplishments/","section":"","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"49cbbfcf1124f111a37fcbac0faeb48e","permalink":"https://yuhan-shen.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"f1f9ddd144499ee8fae766f749d13434","permalink":"https://yuhan-shen.github.io/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/featured/","section":"","summary":"","tags":null,"title":"Featured Publications","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"97f061f6e721d99ff473e04294bd8fcf","permalink":"https://yuhan-shen.github.io/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tags/","section":"","summary":"","tags":null,"title":"Popular Topics","type":"page"},{"authors":["Yuhan Shen","Lu Wang","Ehsan Elhamifar"],"categories":[],"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"96c130abf8fb5e9c540db2b6a9ac7152","permalink":"https://yuhan-shen.github.io/publication/cvpr2021/","publishdate":"2021-03-01T00:39:49-04:00","relpermalink":"/publication/cvpr2021/","section":"publication","summary":"","tags":[],"title":"Learning to Segment Actions from Visual and Language Instructions via Differentiable Weak Sequence Alignment","type":"publication"},{"authors":[],"categories":[],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"5bff8d7ad55ab554b46cacc748d53173","permalink":"https://yuhan-shen.github.io/post/cvpr2021/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/post/cvpr2021/","section":"post","summary":"","tags":[],"title":"[2021/03] One paper is accepted to CVPR 2021 as oral presentation","type":"post"},{"authors":["Kexin He","Yuhan Shen","Wei-Qiang Zhang","Jia Liu"],"categories":[],"content":"","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"8b3a9f1fc3dc8be6b592313b677f37d4","permalink":"https://yuhan-shen.github.io/publication/icassp2020/","publishdate":"2020-05-14T00:39:49-04:00","relpermalink":"/publication/icassp2020/","section":"publication","summary":"Audio tagging aims to predict whether certain acoustic events occur in the audio clips. Due to the difficulty and huge cost of obtaining manually labeled data with high confidence, researchers begin to focus on audio tagging using a small set of manually-labeled data, and a larger set of noisy-labeled data. In addition, audio tagging is a sparse multi-label classification task, where only a small number of acoustic events may occur in an audio clip. In this paper, we propose a staged training strategy to deal with the noisy label, and adopt a sigmoid-sparsemax multi-activation structure to deal with the sparse multi-label classification. This paper is an improvement and extension of our previous work for participation in Task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge. We evaluate our methods on the identical task, and achieve state-of-the-art performance, with an lwlrap score of 0.7591 on official evaluation dataset.","tags":[],"title":"Staged Training Strategy and Multi-Activation for Audio Tagging with Noisy and Sparse Multi-Label Data","type":"publication"},{"authors":[],"categories":[],"content":"","date":1579824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579824000,"objectID":"f948b5dca91a3b44f834fa224325a040","permalink":"https://yuhan-shen.github.io/post/icassp2020/","publishdate":"2020-01-24T00:00:00Z","relpermalink":"/post/icassp2020/","section":"post","summary":"","tags":[],"title":"[2020/01] One paper is accepted to ICASSP 2020","type":"post"},{"authors":[],"categories":[],"content":"","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"20eabdfc26ee016b6504361666b48522","permalink":"https://yuhan-shen.github.io/post/dcase2019_workshop/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/post/dcase2019_workshop/","section":"post","summary":"","tags":[],"title":"[2019/10] Travelled to New York City to attend DCASE Workshop 2019","type":"post"},{"authors":["Kexin He","Yuhan Shen","Wei-Qiang Zhang"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"538ec5ece6e936416b988c0f8a85e1dd","permalink":"https://yuhan-shen.github.io/content/publication/he-2019/","publishdate":"2019-10-28T21:09:54.387623Z","relpermalink":"/content/publication/he-2019/","section":"content","summary":"In this paper, we describe our system for the Task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge: Audio tagging with noisy labels and minimal supervision. This task provides a small amount of verified data (curated data) and a larger quantity of unverified data (noisy data) as training data. Each audio clip contains one or more sound events, so it can be considered as a multi-label audio classification task. To tackle this problem, we mainly use four strategies. The first is a sigmoid-softmax activation to deal with so-called sparse multi-label classification. The second is a staged training strategy to learn from noisy data. The third is a post-processing method that normalizes output scores for each sound class. The last is an ensemble method that averages models learned with multiple neural networks and various acoustic features. All of the above strategies contribute to our system significantly. Our final system achieved labelweighted label-ranking average precision (lwlrap) scores of 0.758 on the private test dataset and 0.742 on the public test dataset, winning the 2nd place in DCASE 2019 Challenge Task 2.","tags":null,"title":"Multiple Neural Networks with Ensemble Method for Audio Tagging with Noisy Labels and Minimal Supervision","type":"content"},{"authors":["Kexin He*","Yuhan Shen*","Wei-Qiang Zhang"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"8fe7f2cafed8b5e408e8992eb82983cd","permalink":"https://yuhan-shen.github.io/publication/dcase2019_he/","publishdate":"2019-10-28T21:09:54.387623Z","relpermalink":"/publication/dcase2019_he/","section":"publication","summary":"In this paper, we describe our system for the Task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2019 Challenge: Audio tagging with noisy labels and minimal supervision. This task provides a small amount of verified data (curated data) and a larger quantity of unverified data (noisy data) as training data. Each audio clip contains one or more sound events, so it can be considered as a multi-label audio classification task. To tackle this problem, we mainly use four strategies. The first is a sigmoid-softmax activation to deal with so-called sparse multi-label classification. The second is a staged training strategy to learn from noisy data. The third is a post-processing method that normalizes output scores for each sound class. The last is an ensemble method that averages models learned with multiple neural networks and various acoustic features. All of the above strategies contribute to our system significantly. Our final system achieved labelweighted label-ranking average precision (lwlrap) scores of 0.758 on the private test dataset and 0.742 on the public test dataset, winning the 2nd place in DCASE 2019 Challenge Task 2.","tags":null,"title":"Multiple Neural Networks with Ensemble Method for Audio Tagging with Noisy Labels and Minimal Supervision","type":"publication"},{"authors":[],"categories":[],"content":"This is my first PhD project. The goal is to summarize the key steps of instructional videos from both visual and language data using unsupervised procedure learning. \n","date":1569702053,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"0647c3b506452801d7096150d6794a39","permalink":"https://yuhan-shen.github.io/project/video_summarization/","publishdate":"2019-09-28T16:20:53-04:00","relpermalink":"/project/video_summarization/","section":"project","summary":"To summarize the key steps of instructional videos from both visual and language data using unsupervised procedure learning. ","tags":[],"title":"Multi-modal Procedure Learning for Video Summarization","type":"project"},{"authors":[],"categories":[],"content":"","date":1569644939,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"274e07a4cdb0449f6dab5a2f3798d117","permalink":"https://yuhan-shen.github.io/post/neu/","publishdate":"2019-09-28T00:28:59-04:00","relpermalink":"/post/neu/","section":"post","summary":"","tags":[],"title":"[2019/09] Starting PhD program at Northeastern University","type":"post"},{"authors":["Yu-Han Shen","Ke-Xin He","Wei-Qiang Zhang"],"categories":[],"content":"","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"10841c6862e2c73e0baf8a38fd5e3d39","permalink":"https://yuhan-shen.github.io/publication/interspeech2019_shen/","publishdate":"2019-09-28T00:39:49-04:00","relpermalink":"/publication/interspeech2019_shen/","section":"publication","summary":"In this paper, we propose a temporal-frequential attention model for sound event detection (SED). Our network learns how to listen with two attention models: a temporal attention model and a frequential attention model. Proposed system learns when to listen using the temporal attention model while it learns where to listen on the frequency axis using the frequential attention model. With these two models, we attempt to make our system pay more attention to important frames or segments and important frequency components for sound event detection. Our proposed method is demonstrated on the task 2 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge and outperforms state-of-the-art methods.","tags":[],"title":"Learning How to Listen: A Temporal-Frequential Attention Model for Sound Event Detection","type":"publication"},{"authors":["Ke-Xin He*","Yu-Han Shen*","Wei-Qiang Zhang"],"categories":[],"content":"","date":1568436901,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"7a6cdc9a91b7cdb06f32fc21a1aba06a","permalink":"https://yuhan-shen.github.io/publication/interspeech2019_he/","publishdate":"2019-09-14T00:55:01-04:00","relpermalink":"/publication/interspeech2019_he/","section":"publication","summary":"Sound event detection with weakly labeled data is considered as a problem of multi-instance learning. And the choice of pooling function is the key to solving this problem. In this paper, we proposed a hierarchical pooling structure to improve the performance of weakly labeled sound event detection system. Proposed pooling structure has made remarkable improvements on three types of pooling function without adding any parameters. Moreover, our system has achieved competitive performance on Task 4 of Detection and Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge using hierarchical pooling structure.","tags":[],"title":"Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection","type":"publication"},{"authors":[],"categories":[],"content":"","date":1566518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"5a1f44a21db693507019a955a3752ff7","permalink":"https://yuhan-shen.github.io/post/dcase2019/","publishdate":"2019-08-23T00:00:00Z","relpermalink":"/post/dcase2019/","section":"post","summary":"","tags":[],"title":"[2019/08] One paper is accepted to DCASE Workshop 2019","type":"post"},{"authors":[],"categories":[],"content":"","date":1563667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"f212621d1ec3d57b4391246d782ecb08","permalink":"https://yuhan-shen.github.io/post/dcase2019_challenge/","publishdate":"2019-07-21T00:00:00Z","relpermalink":"/post/dcase2019_challenge/","section":"post","summary":"","tags":[],"title":"[2019/07] Won the 2nd place in Task 2 of DCASE Challenge 2019","type":"post"},{"authors":[],"categories":[],"content":"This is a task of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. It aims to develop a well-performed audio tagging system using a small amount of manually-labeled data and a large quantity of noisy-labeled data. We took part in this competition and won the 2nd place. To achieve state-of-the-art performance, we mainly adopted the following strategies:\n We used mixup and SpecAugment for data augmentation. We designed a sigmoid-softmax activation structure to deal with sparse multi-label classification. We proposed a staged training strategy to learn from nose data. We applied post-processing method that normalizes output scores for each sound class. We adopted ensemble method that averages models learned with multiple neural networks and acoustic features.   ","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"315c8e78b70fb969c42d503130907ac0","permalink":"https://yuhan-shen.github.io/project/audio_tagging/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/project/audio_tagging/","section":"project","summary":"Classified multi-label audio clips using a small amount of manually-labeled data and a large quantity of noisy-labeled data. ","tags":["Audio"],"title":"Audio Tagging with Noisy Labels and Minimal Supervision","type":"project"},{"authors":[],"categories":[],"content":"","date":1560729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"33fc739bcc711e6fc1ae3cf2dc3b4845","permalink":"https://yuhan-shen.github.io/post/interspeech2019/","publishdate":"2019-06-17T00:00:00Z","relpermalink":"/post/interspeech2019/","section":"post","summary":"","tags":[],"title":"[2019/06] Two papers are accepted to Interspeech 2019","type":"post"},{"authors":[],"categories":[],"content":"My contributions and achievements go as follows:\n Proposed a bi-domain (on both time domain and frequency domain) attention model for rare sound event detection. Proposed a hierarchical pooling structure for weakly-labeled sound event detection. Outperformed state-of-the-art methods on evaluation dataset of Task 2 and Task 4 of Detection and Classification of Acoustic Events and Scenes (DCASE) 2017. Two Papers published in Proceedings of Interspeech 2019 as first or co-first author.   ","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"6df55ffda114cc5d9eecc78d3fa1551e","permalink":"https://yuhan-shen.github.io/project/sound_event_detection/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/project/sound_event_detection/","section":"project","summary":"Achieved state-of-the-art performance on rare sound event detection and weakly-labeled sound event detection.","tags":["Audio"],"title":"Research on Sound Event Detection","type":"project"},{"authors":["Yu-Han Shen","Ke-Xin He","Wei-Qiang Zhang"],"categories":[],"content":"","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569703119,"objectID":"ed9b9529c6164cce139084c619b055bd","permalink":"https://yuhan-shen.github.io/publication/isspit2018/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/publication/isspit2018/","section":"publication","summary":"In this paper, we propose a method for home activity monitoring. We demonstrate our model on dataset of Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify multi-channel audios into one of the provided pre-defined classes. All of these classes are daily activities performed in a home environment. To tackle this task, we propose a gated convolutional neural network with segment-level attention mechanism (SAM-GCNN). The proposed framework is a convolutional model with two auxiliary modules: a gated convolutional neural network and a segment-level attention mechanism. Furthermore, we adopted model ensemble to enhance the capability of generalization of our model. We evaluated our work on the development dataset of DCASE 2018 Task 5 and achieved competitive performance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%, compared with the convolutional baseline system.","tags":[],"title":"SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level Attention Mechanism for Home Activity Monitoring","type":"publication"}]