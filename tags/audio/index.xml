<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Audio | Yuhan Shen</title>
    <link>https://yuhan-shen.github.io/tags/audio/</link>
      <atom:link href="https://yuhan-shen.github.io/tags/audio/index.xml" rel="self" type="application/rss+xml" />
    <description>Audio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Yuhan Shen</copyright><lastBuildDate>Mon, 01 Jul 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yuhan-shen.github.io/img/icon-192.png</url>
      <title>Audio</title>
      <link>https://yuhan-shen.github.io/tags/audio/</link>
    </image>
    
    <item>
      <title>Audio Tagging with Noisy Labels and Minimal Supervision</title>
      <link>https://yuhan-shen.github.io/project/audio_tagging/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/project/audio_tagging/</guid>
      <description>&lt;p&gt;&lt;DIV align = &#34;justify&#34;&gt;
This is a task of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge 2019. It aims to develop a well-performed audio tagging system using a small amount of manually-labeled data and a large quantity of noisy-labeled data. We took part in this competition and won the 2nd place. To achieve state-of-the-art performance, we mainly adopted the following strategies:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We used mixup and SpecAugment for data augmentation.&lt;/li&gt;
&lt;li&gt;We designed a sigmoid-softmax activation structure to deal with sparse multi-label classification.&lt;/li&gt;
&lt;li&gt;We proposed a staged training strategy to learn from nose data.&lt;/li&gt;
&lt;li&gt;We applied post-processing method that normalizes output scores for each sound class.&lt;/li&gt;
&lt;li&gt;We adopted ensemble method that averages models learned with multiple neural networks and acoustic features.
&lt;/DIV&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Research on Sound Event Detection</title>
      <link>https://yuhan-shen.github.io/project/sound_event_detection/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://yuhan-shen.github.io/project/sound_event_detection/</guid>
      <description>&lt;p&gt;&lt;DIV align = &#34;justify&#34;&gt;
My contributions and achievements go as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proposed a bi-domain (on both time domain and frequency domain) attention model for rare sound event detection.&lt;/li&gt;
&lt;li&gt;Proposed a hierarchical pooling structure for weakly-labeled sound event detection.&lt;/li&gt;
&lt;li&gt;Outperformed state-of-the-art methods on evaluation dataset of Task 2 and Task 4 of Detection and Classification of Acoustic Events and Scenes (DCASE) 2017.&lt;/li&gt;
&lt;li&gt;Two Papers published in Proceedings of Interspeech 2019 as first or co-first author.
&lt;/DIV&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
